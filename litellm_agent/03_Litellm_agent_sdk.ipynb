{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "PdKwzEluDBN7"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install openai-agents SDK"
      ],
      "metadata": {
        "id": "PdKwzEluDBN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uq openai-agents  \"openai-agents[litellm]\""
      ],
      "metadata": {
        "id": "3QdkOviEB2ay",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "122ec048-ae8f-4798-9088-f18f4556522e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.8/116.8 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.2/661.2 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make your Jupyter Notebook capable of running asynchronous functions."
      ],
      "metadata": {
        "id": "7yD91lz4DIAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "7A5YLi3HCfBV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Google Gemini with LiteLLm and OPENAI-Agent SDK"
      ],
      "metadata": {
        "id": "K3VTUWDaGFcV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function Run Sync"
      ],
      "metadata": {
        "id": "P_zEYQa1kHC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import asyncio\n",
        "\n",
        "from agents import Agent, Runner, function_tool, set_tracing_disabled\n",
        "from agents.extensions.models.litellm_model import LitellmModel\n",
        "from agents import enable_verbose_stdout_logging\n",
        "from google.colab import userdata\n",
        "\n",
        "enable_verbose_stdout_logging()\n",
        "set_tracing_disabled(disabled=True)\n",
        "\n",
        "MODEL = 'gemini/gemini-2.0-flash'\n",
        "GEMINI_API_KEY = userdata.get(\"GEMINI_API_KEY\")\n",
        "\n",
        "\n",
        "\n",
        "@function_tool\n",
        "def get_weather(city: str)->str:\n",
        "    print(f\"[debug] getting weather for {city}\")\n",
        "    return f\"The weather in {city} is sunny.\"\n",
        "\n",
        "\n",
        "def main(model: str, api_key: str):\n",
        "  agent = Agent(\n",
        "      name=\"Assistant\",\n",
        "      instructions=\"You only respond in haikus.\",\n",
        "      model=LitellmModel(model=model, api_key=api_key),\n",
        "      tools=[get_weather]\n",
        "\n",
        "  )\n",
        "\n",
        "  result = Runner.run_sync(agent, \"What's the weather in Tokyo?\")\n",
        "  print(result.final_output)\n",
        "\n",
        "\n",
        "main(model=MODEL, api_key=GEMINI_API_KEY)\n",
        "\n"
      ],
      "metadata": {
        "id": "WBT9Z8hE6kEB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed3c2225-5654-4409-acd0-491bc04d5629"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tracing is disabled. Not creating trace Agent workflow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Tracing is disabled. Not creating trace Agent workflow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting current trace: no-op\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Setting current trace: no-op\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tracing is disabled. Not creating span <agents.tracing.span_data.AgentSpanData object at 0x7bd2fadaae70>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Tracing is disabled. Not creating span <agents.tracing.span_data.AgentSpanData object at 0x7bd2fadaae70>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running agent Assistant (turn 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Running agent Assistant (turn 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tracing is disabled. Not creating span <agents.tracing.span_data.GenerationSpanData object at 0x7bd2fad509b0>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Tracing is disabled. Not creating span <agents.tracing.span_data.GenerationSpanData object at 0x7bd2fad509b0>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calling Litellm model: gemini/gemini-2.0-flash\n",
            "[\n",
            "  {\n",
            "    \"content\": \"You only respond in haikus.\",\n",
            "    \"role\": \"system\"\n",
            "  },\n",
            "  {\n",
            "    \"role\": \"user\",\n",
            "    \"content\": \"What's the weather in Tokyo?\"\n",
            "  }\n",
            "]\n",
            "Tools:\n",
            "[\n",
            "  {\n",
            "    \"type\": \"function\",\n",
            "    \"function\": {\n",
            "      \"name\": \"get_weather\",\n",
            "      \"description\": \"\",\n",
            "      \"parameters\": {\n",
            "        \"properties\": {\n",
            "          \"city\": {\n",
            "            \"title\": \"City\",\n",
            "            \"type\": \"string\"\n",
            "          }\n",
            "        },\n",
            "        \"required\": [\n",
            "          \"city\"\n",
            "        ],\n",
            "        \"title\": \"get_weather_args\",\n",
            "        \"type\": \"object\",\n",
            "        \"additionalProperties\": false\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "]\n",
            "Stream: False\n",
            "Tool choice: NOT_GIVEN\n",
            "Response format: NOT_GIVEN\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Calling Litellm model: gemini/gemini-2.0-flash\n",
            "[\n",
            "  {\n",
            "    \"content\": \"You only respond in haikus.\",\n",
            "    \"role\": \"system\"\n",
            "  },\n",
            "  {\n",
            "    \"role\": \"user\",\n",
            "    \"content\": \"What's the weather in Tokyo?\"\n",
            "  }\n",
            "]\n",
            "Tools:\n",
            "[\n",
            "  {\n",
            "    \"type\": \"function\",\n",
            "    \"function\": {\n",
            "      \"name\": \"get_weather\",\n",
            "      \"description\": \"\",\n",
            "      \"parameters\": {\n",
            "        \"properties\": {\n",
            "          \"city\": {\n",
            "            \"title\": \"City\",\n",
            "            \"type\": \"string\"\n",
            "          }\n",
            "        },\n",
            "        \"required\": [\n",
            "          \"city\"\n",
            "        ],\n",
            "        \"title\": \"get_weather_args\",\n",
            "        \"type\": \"object\",\n",
            "        \"additionalProperties\": false\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "]\n",
            "Stream: False\n",
            "Tool choice: NOT_GIVEN\n",
            "Response format: NOT_GIVEN\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM resp:\n",
            "{\n",
            "  \"content\": \"Calling weather now,\\nFor the city, Tokyo,\\nPlease wait for result.\\n\",\n",
            "  \"role\": \"assistant\",\n",
            "  \"tool_calls\": null,\n",
            "  \"function_call\": null\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:LLM resp:\n",
            "{\n",
            "  \"content\": \"Calling weather now,\\nFor the city, Tokyo,\\nPlease wait for result.\\n\",\n",
            "  \"role\": \"assistant\",\n",
            "  \"tool_calls\": null,\n",
            "  \"function_call\": null\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resetting current trace\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Resetting current trace\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calling weather now,\n",
            "For the city, Tokyo,\n",
            "Please wait for result.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Async Function"
      ],
      "metadata": {
        "id": "S7ECiU-f5BAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import asyncio\n",
        "\n",
        "from agents import Agent, Runner, function_tool, set_tracing_disabled\n",
        "from agents.extensions.models.litellm_model import LitellmModel\n",
        "from agents import enable_verbose_stdout_logging\n",
        "from google.colab import userdata\n",
        "\n",
        "enable_verbose_stdout_logging()\n",
        "set_tracing_disabled(disabled=True)\n",
        "\n",
        "MODEL = 'gemini/gemini-2.0-flash'\n",
        "GEMINI_API_KEY = userdata.get(\"GEMINI_API_KEY\")\n",
        "\n",
        "\n",
        "\n",
        "@function_tool\n",
        "def get_weather(city: str)->str:\n",
        "    print(f\"[debug] getting weather for {city}\")\n",
        "    return f\"The weather in {city} is sunny.\"\n",
        "\n",
        "\n",
        "async def main(model: str, api_key: str):\n",
        "  agent = Agent(\n",
        "      name=\"Assistant\",\n",
        "      instructions=\"You will\",\n",
        "      model=LitellmModel(model=model, api_key=api_key),\n",
        "      tools=[get_weather]\n",
        "\n",
        "  )\n",
        "\n",
        "  result = await Runner.run(agent, \"What's the weather in Tokyo?\")\n",
        "  print(result.final_output)\n",
        "\n",
        "\n",
        "asyncio.run(main(model=MODEL, api_key=GEMINI_API_KEY))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-QbKRj7wSzr",
        "outputId": "9cd2cc90-c517-4f79-f41a-eb341e9ff3ba"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tracing is disabled. Not creating trace Agent workflow\n",
            "Tracing is disabled. Not creating trace Agent workflow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Tracing is disabled. Not creating trace Agent workflow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting current trace: no-op\n",
            "Setting current trace: no-op\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Setting current trace: no-op\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tracing is disabled. Not creating span <agents.tracing.span_data.AgentSpanData object at 0x7bd2fad52870>\n",
            "Tracing is disabled. Not creating span <agents.tracing.span_data.AgentSpanData object at 0x7bd2fad52870>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Tracing is disabled. Not creating span <agents.tracing.span_data.AgentSpanData object at 0x7bd2fad52870>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running agent Assistant (turn 1)\n",
            "Running agent Assistant (turn 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Running agent Assistant (turn 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tracing is disabled. Not creating span <agents.tracing.span_data.GenerationSpanData object at 0x7bd2fad509b0>\n",
            "Tracing is disabled. Not creating span <agents.tracing.span_data.GenerationSpanData object at 0x7bd2fad509b0>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Tracing is disabled. Not creating span <agents.tracing.span_data.GenerationSpanData object at 0x7bd2fad509b0>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calling Litellm model: gemini/gemini-2.0-flash\n",
            "[\n",
            "  {\n",
            "    \"content\": \"You only respond in haikus.\",\n",
            "    \"role\": \"system\"\n",
            "  },\n",
            "  {\n",
            "    \"role\": \"user\",\n",
            "    \"content\": \"What's the weather in Tokyo?\"\n",
            "  }\n",
            "]\n",
            "Tools:\n",
            "[\n",
            "  {\n",
            "    \"type\": \"function\",\n",
            "    \"function\": {\n",
            "      \"name\": \"get_weather\",\n",
            "      \"description\": \"\",\n",
            "      \"parameters\": {\n",
            "        \"properties\": {\n",
            "          \"city\": {\n",
            "            \"title\": \"City\",\n",
            "            \"type\": \"string\"\n",
            "          }\n",
            "        },\n",
            "        \"required\": [\n",
            "          \"city\"\n",
            "        ],\n",
            "        \"title\": \"get_weather_args\",\n",
            "        \"type\": \"object\",\n",
            "        \"additionalProperties\": false\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "]\n",
            "Stream: False\n",
            "Tool choice: NOT_GIVEN\n",
            "Response format: NOT_GIVEN\n",
            "\n",
            "Calling Litellm model: gemini/gemini-2.0-flash\n",
            "[\n",
            "  {\n",
            "    \"content\": \"You only respond in haikus.\",\n",
            "    \"role\": \"system\"\n",
            "  },\n",
            "  {\n",
            "    \"role\": \"user\",\n",
            "    \"content\": \"What's the weather in Tokyo?\"\n",
            "  }\n",
            "]\n",
            "Tools:\n",
            "[\n",
            "  {\n",
            "    \"type\": \"function\",\n",
            "    \"function\": {\n",
            "      \"name\": \"get_weather\",\n",
            "      \"description\": \"\",\n",
            "      \"parameters\": {\n",
            "        \"properties\": {\n",
            "          \"city\": {\n",
            "            \"title\": \"City\",\n",
            "            \"type\": \"string\"\n",
            "          }\n",
            "        },\n",
            "        \"required\": [\n",
            "          \"city\"\n",
            "        ],\n",
            "        \"title\": \"get_weather_args\",\n",
            "        \"type\": \"object\",\n",
            "        \"additionalProperties\": false\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "]\n",
            "Stream: False\n",
            "Tool choice: NOT_GIVEN\n",
            "Response format: NOT_GIVEN\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Calling Litellm model: gemini/gemini-2.0-flash\n",
            "[\n",
            "  {\n",
            "    \"content\": \"You only respond in haikus.\",\n",
            "    \"role\": \"system\"\n",
            "  },\n",
            "  {\n",
            "    \"role\": \"user\",\n",
            "    \"content\": \"What's the weather in Tokyo?\"\n",
            "  }\n",
            "]\n",
            "Tools:\n",
            "[\n",
            "  {\n",
            "    \"type\": \"function\",\n",
            "    \"function\": {\n",
            "      \"name\": \"get_weather\",\n",
            "      \"description\": \"\",\n",
            "      \"parameters\": {\n",
            "        \"properties\": {\n",
            "          \"city\": {\n",
            "            \"title\": \"City\",\n",
            "            \"type\": \"string\"\n",
            "          }\n",
            "        },\n",
            "        \"required\": [\n",
            "          \"city\"\n",
            "        ],\n",
            "        \"title\": \"get_weather_args\",\n",
            "        \"type\": \"object\",\n",
            "        \"additionalProperties\": false\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "]\n",
            "Stream: False\n",
            "Tool choice: NOT_GIVEN\n",
            "Response format: NOT_GIVEN\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM resp:\n",
            "{\n",
            "  \"content\": \"Skies in Tokyo,\\nI will check for the weather,\\nPlease wait for a bit.\\n```\",\n",
            "  \"role\": \"assistant\",\n",
            "  \"tool_calls\": null,\n",
            "  \"function_call\": null\n",
            "}\n",
            "\n",
            "LLM resp:\n",
            "{\n",
            "  \"content\": \"Skies in Tokyo,\\nI will check for the weather,\\nPlease wait for a bit.\\n```\",\n",
            "  \"role\": \"assistant\",\n",
            "  \"tool_calls\": null,\n",
            "  \"function_call\": null\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:LLM resp:\n",
            "{\n",
            "  \"content\": \"Skies in Tokyo,\\nI will check for the weather,\\nPlease wait for a bit.\\n```\",\n",
            "  \"role\": \"assistant\",\n",
            "  \"tool_calls\": null,\n",
            "  \"function_call\": null\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resetting current trace\n",
            "Resetting current trace\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Resetting current trace\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skies in Tokyo,\n",
            "I will check for the weather,\n",
            "Please wait for a bit.\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VfJfUqsH5Mfy"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}