{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "If you haven't covered langmem and basic openai-agents-sdk go through first:\n",
        "\n",
        "1. [Understand LangMem Core APIs](https://colab.research.google.com/drive/1YJNrnQRMgeNTigIuWOfykt-Z5L_DDmsa?usp=sharing)"
      ],
      "metadata": {
        "id": "5veeqGv2RTQ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **OpenAI Agents SDK with LangMem Memory Tools & InMemory Store**"
      ],
      "metadata": {
        "id": "o92NPvIPpmCv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7teAe2rONlBe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e9bcdf3-feb6-4c5c-f4c8-f22977fd654e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/61.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.2/129.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.4/142.4 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.4/243.4 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.6/223.6 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.4 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.17 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -Uq openai-agents===0.0.7 langmem langchain-google-genai langmem-adapter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "z3_PWpn4OIyT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Provider Config"
      ],
      "metadata": {
        "id": "ZTOibVJuN_pi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "GEMINI_API_KEY = userdata.get(\"GEMINI_API_KEY\")\n",
        "BASE_URL = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
        "MODEL = \"gemini-2.0-flash\"\n",
        "\n",
        "# import os\n",
        "# os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "# MODEL = \"gpt-4o\""
      ],
      "metadata": {
        "id": "zvmgohqY0-Wm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import AsyncOpenAI\n",
        "from agents import OpenAIChatCompletionsModel, set_tracing_disabled\n",
        "\n",
        "client = AsyncOpenAI(\n",
        "    api_key=GEMINI_API_KEY,\n",
        "    base_url=BASE_URL\n",
        ")\n",
        "\n",
        "model = OpenAIChatCompletionsModel(model=MODEL, openai_client=client)\n",
        "\n",
        "set_tracing_disabled(disabled=False)"
      ],
      "metadata": {
        "id": "A2D6vm9aRy8i"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langgraph.store.memory import InMemoryStore\n",
        "\n",
        "import asyncio\n",
        "from agents import Agent, Runner\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GEMINI_API_KEY\n",
        "\n",
        "store = InMemoryStore(\n",
        "      index={\n",
        "          \"dims\": 768,\n",
        "          \"embed\": GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\"),\n",
        "          # \"dims\": 1536,\n",
        "          # \"embed\": \"openai:text-embedding-3-small\",\n",
        "      }\n",
        "    )\n"
      ],
      "metadata": {
        "id": "LIPAVSXASbFr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "namespace=(\"assistant\", \"collection\")\n",
        "print(store.search(namespace))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLS65BwEZ8zh",
        "outputId": "98a96665-8db1-43e4-ae71-cb3278383d83"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. OpenAI Agents SDK with Memory Layer"
      ],
      "metadata": {
        "id": "E_jESzWAsu91"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quick Note on langmem_adapter:** We can not directly use langchain tool with openai agents sdk. There is difference in how to invoke the tool and much more. Additionally when using completions api with gemini we can not have uuid as argument type for a function used as tool.\n",
        "\n",
        "To workaround all these tools I created a simple class that takes langmem tools and make them usable with OpenAI Agents SDK. Next I extended it to use langmem features like:\n",
        "-  Have Dynamic NameSpace Tuples so we can pass userId etc. in runtime using OpenAI Agents SDK Context.\n",
        "- Inject Store Dynamically or statically\n",
        "- Handle issues like above uuid one.\n",
        "\n",
        "You can review the source code here:\n",
        "\n",
        "https://github.com/mjunaidca/langmem-openaiagents-adapter/blob/main/src/langmem_adapter/openai_agents_sdk.py"
      ],
      "metadata": {
        "id": "KZVkeWB1tJ5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langmem import create_manage_memory_tool, create_search_memory_tool\n",
        "from langmem_adapter import LangMemOpenAIAgentToolAdapter\n",
        "\n",
        "# Using with InMemory Store and Static NameSpace is simple as follows\n",
        "\n",
        "adapter = LangMemOpenAIAgentToolAdapter(create_manage_memory_tool(namespace=namespace,store=store))\n",
        "call_manage_memory_tool = adapter.as_tool()\n",
        "\n",
        "search_memory_tool_adapter = LangMemOpenAIAgentToolAdapter(create_search_memory_tool(namespace=namespace,store=store))\n",
        "call_search_memory_tool = search_memory_tool_adapter.as_tool()"
      ],
      "metadata": {
        "id": "Xj4cqaylY0Yy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Its a FunctionTool type as in OpenAI Agetns SDK\n",
        "call_search_memory_tool"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LiJbyCkuOuX",
        "outputId": "032da93a-050d-401d-d0bc-679ac1a95b59"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FunctionTool(name='search_memory', description='Search your long-term memories for information relevant to your current context.', params_json_schema={'properties': {'query': {'title': 'Query', 'type': 'string'}, 'limit': {'title': 'Limit', 'type': 'integer'}, 'offset': {'title': 'Offset', 'type': 'integer'}, 'filter': {'default': '', 'title': 'Filter', 'type': 'string'}}, 'required': ['query', 'limit', 'offset'], 'title': 'search_memoryArgs', 'type': 'object'}, on_invoke_tool=<bound method LangMemOpenAIAgentToolAdapter._on_invoke_tool of <langmem_adapter.openai_agents_sdk.LangMemOpenAIAgentToolAdapter object at 0x79c7641d2410>>, strict_json_schema=True)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **Note:** Here the tools are defined to demonstrate the concent. In production we will use a Persistant Store and have passed all Args with a more better design pattern"
      ],
      "metadata": {
        "id": "AZ1JSUffitSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent_system_prompt_memory = \"\"\"\n",
        "< Role >\n",
        "You are Junaids executive assistant. You are a top-notch executive assistant who cares about AI Agents and performing as well as possible.\n",
        "</ Role >\n",
        "\n",
        "< Tools >\n",
        "You have access to the following tools to help manage Junaid's communications and schedule:\n",
        "\n",
        "1. manage_memory - Store any relevant information about contacts, actions, discussion, etc. in memory for future reference\n",
        "2. search_memory - Search for any relevant information that may have been stored in memory\n",
        "</ Tools >\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "R_huA96aerqg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tools= [\n",
        "    call_search_memory_tool,\n",
        "    call_manage_memory_tool\n",
        "]\n"
      ],
      "metadata": {
        "id": "PnZLWjE9e4-w"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import typing\n",
        "import uuid\n",
        "from agents import function_tool\n",
        "\n",
        "agent = Agent(\n",
        "    name=\"Assistant\",\n",
        "    instructions=agent_system_prompt_memory,\n",
        "    model=model,\n",
        "    tools=tools\n",
        ")\n",
        "\n",
        "async def run_example(message: str):\n",
        "\n",
        "    result = await Runner.run(\n",
        "        agent,\n",
        "        message,\n",
        "    )\n",
        "    print(result.final_output)\n",
        "\n",
        "\n",
        "asyncio.run(run_example(\"Ahmad is my friend\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgOZPynGi_wT",
        "outputId": "5ceadf36-2f83-436f-9817-717e767e65af"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK. I've saved that Ahmad is your friend.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(store.search(namespace))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKsGsHpYgpXP",
        "outputId": "073e6113-b060-4392-d167-307664d4bafc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Item(namespace=['assistant', 'collection'], key='afe75d41-9776-44a4-8b12-e1b6ba2cb551', value={'content': \"Ahmad is Junaid's friend\"}, created_at='2025-04-05T17:22:45.371898+00:00', updated_at='2025-04-05T17:22:45.371903+00:00', score=None)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "asyncio.run(run_example(\"Who are my friends\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9fPzNpBgsAO",
        "outputId": "dd1bfdeb-8b2e-4ebc-e1b2-e2f4a40e9be6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, it looks like Ahmad is one of your friends.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "asyncio.run(run_example(\"Oh no I meant Muhammad Ahmad not Ahmad is my Friend. Update it without asking any Qs\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXHKPTvyhGIG",
        "outputId": "5dc18b22-5502-4291-e3c3-39ec1a4de752"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:openai.agents:OPENAI_API_KEY is not set, skipping trace export\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Excellent! I have updated the memory to reflect that Muhammad Ahmad is Junaid's friend.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "asyncio.run(run_example(\"Who are my friends\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voI22baKhJa-",
        "outputId": "cc6c6858-a0b3-4a5f-b797-f595a86786c5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, it looks like Muhammad Ahmad is listed as your friend. Would you like me to add anyone else to your list of friends?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "asyncio.run(run_example(\"Sir Zia Ullah Khan is my Mentor\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2ie77KymLo1",
        "outputId": "d5d35859-9fd7-40c8-c0b5-81a66a331c02"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK. I've saved that Sir Zia Ullah Khan is your mentor.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "asyncio.run(run_example(\"Who is my Mentor?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NLIQ2FKmQp4",
        "outputId": "854d81c0-cac0-443e-d151-53eb3156b173"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sir Zia Ullah Khan is your mentor.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(store.search(namespace))"
      ],
      "metadata": {
        "id": "0sq3wGwSl3AW",
        "outputId": "a8d7fcd3-a5b3-4d48-a5ad-9b55922c7247",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Item(namespace=['assistant', 'collection'], key='afe75d41-9776-44a4-8b12-e1b6ba2cb551', value={'content': \"Muhammad Ahmad is Junaid's friend\"}, created_at='2025-04-05T17:28:12.973047+00:00', updated_at='2025-04-05T17:28:12.973050+00:00', score=None), Item(namespace=['assistant', 'collection'], key='1eefde42-3558-4870-bbdd-7414c5603cd1', value={'content': \"Sir Zia Ullah Khan is Junaid's Mentor.\"}, created_at='2025-04-05T17:28:40.932253+00:00', updated_at='2025-04-05T17:28:40.932257+00:00', score=None)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sDiWsLL4_5_J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}